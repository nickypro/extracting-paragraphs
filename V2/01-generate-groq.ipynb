{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.42it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  3.39it/s]:02<00:55,  2.92s/it]\n",
      "100%|██████████| 10/10 [00:02<00:00,  3.58it/s]:05<00:52,  2.94s/it]\n",
      "100%|██████████| 10/10 [00:24<00:00,  2.45s/it]:08<00:48,  2.87s/it]\n",
      "100%|██████████| 10/10 [00:24<00:00,  2.44s/it]:33<03:02, 11.41s/it]\n",
      "100%|██████████| 10/10 [00:24<00:00,  2.43s/it]:57<04:01, 16.10s/it]\n",
      "100%|██████████| 10/10 [00:24<00:00,  2.46s/it]:21<04:24, 18.89s/it]\n",
      "100%|██████████| 10/10 [00:24<00:00,  2.48s/it]:46<04:29, 20.76s/it]\n",
      "100%|██████████| 10/10 [00:24<00:00,  2.43s/it]:11<04:24, 22.05s/it]\n",
      "100%|██████████| 10/10 [00:24<00:00,  2.44s/it]:35<04:10, 22.77s/it]\n",
      "100%|██████████| 10/10 [00:24<00:00,  2.45s/it]3:00<03:52, 23.26s/it]\n",
      "100%|██████████| 10/10 [00:24<00:00,  2.44s/it]3:24<03:32, 23.65s/it]\n",
      "100%|██████████| 10/10 [00:24<00:00,  2.44s/it]3:48<03:11, 23.88s/it]\n",
      "100%|██████████| 10/10 [00:24<00:00,  2.44s/it]4:13<02:48, 24.05s/it]\n",
      "100%|██████████| 10/10 [00:24<00:00,  2.44s/it]4:37<02:24, 24.15s/it]\n",
      "100%|██████████| 10/10 [00:24<00:00,  2.45s/it]5:02<02:01, 24.22s/it]\n",
      "100%|██████████| 10/10 [00:24<00:00,  2.46s/it]5:26<01:37, 24.30s/it]\n",
      "100%|██████████| 10/10 [00:24<00:00,  2.45s/it]5:51<01:13, 24.39s/it]\n",
      "100%|██████████| 10/10 [00:24<00:00,  2.44s/it]6:15<00:48, 24.41s/it]\n",
      "100%|██████████| 10/10 [00:24<00:00,  2.44s/it]6:40<00:24, 24.42s/it]\n",
      "Generating responses: 100%|██████████| 20/20 [07:04<00:00, 21.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from groq import Groq\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq(\n",
    "    api_key=\"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\",\n",
    ")\n",
    "\n",
    "# Load prompts from file\n",
    "folder = \"../data/llama9b\"\n",
    "version = \"V2.1\"\n",
    "num_copies = 10\n",
    "SKIP_TEXTS = 0\n",
    "with open(f\"{folder}/{version}_prompts.txt\", \"r\") as f:\n",
    "    prompts = f.read().splitlines()\n",
    "\n",
    "# Function to format input for Gemma 2 9B\n",
    "def format_input(prompt):\n",
    "    return f\"<bos><start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>assistant\\n\"\n",
    "\n",
    "# Generate responses and save to JSONL\n",
    "with open(f\"{folder}/{version}_orig_generation.jsonl\", \"a\") as outfile:\n",
    "    for prompt in tqdm(prompts[SKIP_TEXTS:], desc=\"Generating responses\"):\n",
    "        for i in tqdm(range(num_copies)):\n",
    "            formatted_input = format_input(prompt)\n",
    "\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt,\n",
    "                    }\n",
    "                ],\n",
    "                # model=\"gemma2-9b-it\",\n",
    "                # model=\"llama-3.2-11b-text-preview\",\n",
    "                model=\"llama-3.2-3b-preview\",\n",
    "                temperature=0.3,\n",
    "            )\n",
    "\n",
    "            response = chat_completion.choices[0].message.content\n",
    "            formatted_full_text = formatted_input + response\n",
    "\n",
    "            result = {\n",
    "                \"text\": prompt,\n",
    "                \"output\": response,\n",
    "                \"formatted_input\": formatted_input,\n",
    "                \"formatted_full_text\": formatted_full_text\n",
    "            }\n",
    "\n",
    "            json.dump(result, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "print(\"Generation complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
